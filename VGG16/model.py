# -*- coding: utf-8 -*-
"""Final-Covid_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JKKlP3U3wC35L3JGo-ngLFRlZb49Eow-
"""

#Import tensorflow
import tensorflow as tf
print(tf.__version__)

physical_devices = tf.config.experimental.list_physical_devices('GPU')
print("physical_devices-------------", len(physical_devices))
tf.config.experimental.set_memory_growth(physical_devices[0], True)
tf.config.experimental.set_memory_growth(physical_devices[1], True)


#Importing the necessary packages
import os
import numpy as np
import pandas as pd
import cv2 as cv
import matplotlib.pyplot as plt
import keras
from keras.models import Sequential
from keras.layers.convolutional import Conv2D
from keras.layers import GlobalAveragePooling2D
from keras.layers.core import Activation, Dropout, Dense
from keras import backend as K
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam
from keras.preprocessing import image
from tensorflow.keras.utils import to_categorical
from keras.applications.vgg16 import VGG16
from keras.layers import Input
from keras.models import Model
from keras.models import load_model
import seaborn as sns
import sklearn.metrics as metrics
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score
from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau, TensorBoard

#Specifying learning rate, epochs and batch size
lr = 0.0001   
epochs = 200
BS = 32

#Dataset loading and preprocessing it
CATEGORIES = ["covid", "normal", "pneumonia"]

def get_dataset(directory):
  data = []
  labels = []
  for category in CATEGORIES:
      path = os.path.join(directory, category)
      for img in os.listdir(path):
          img_path = os.path.join(path,img)
          im = cv.imread(img_path)
          im = cv.resize(im, (224,224))
          im = np.array(im)/255.0
          
          data.append(im)
          labels.append(category)
      
  lb = LabelBinarizer()
  labels = lb.fit_transform(labels)

  data = np.array(data, dtype="float32") 
  labels = np.array(labels)
  return data, labels
  
#Training and Testing data  
directory = 'path to the train directory'
x_train, y_train  = get_dataset(directory)
#print(x_train.shape, y_train.shape)
x_test, y_test = get_dataset('path to the test directory')

#Data Augmentation
aug = ImageDataGenerator(
    rotation_range=25, width_shift_range=0.1,
    height_shift_range=0.1, shear_range=0.2, 
    zoom_range=0.2,horizontal_flip=True, 
    fill_mode="nearest")

#Five fold cross validation and model compiling and training
n_folds = 5
acc_per_fold = []
loss_per_fold = []
fold_no = 1

kf = KFold(n_splits=n_folds, shuffle=True, random_state=0)
for i, (tr_idx, val_idx) in enumerate(kf.split(x_train,y_train)):
    #print(len(tr_idx), len(val_idx))
    x_tr, y_tr = x_train[tr_idx], y_train[tr_idx]
    x_val, y_val = x_train[val_idx], y_train[val_idx]

    basemodel = VGG16(include_top=False, weights='imagenet', input_shape=(224,224,3))
    headmodel = basemodel.output
    headmodel = GlobalAveragePooling2D()(headmodel)
    headmodel = Dense(512, activation="relu")(headmodel)
    headmodel = Dropout(0.5)(headmodel)
    headmodel = Dense(64, activation="relu")(headmodel)
    headmodel = Dropout(0.5)(headmodel)
    headmodel = Dense(3, activation="softmax")(headmodel)

    model = Model(inputs=basemodel.input, outputs=headmodel)
    
    for layer in basemodel.layers:
        layer.trainable = False
        
    opt = Adam(learning_rate=lr, decay=lr/epochs)
    model.compile(optimizer = opt, loss='binary_crossentropy', metrics=["accuracy"])
    
    mc_path = 'path to the folder/vgg16-models/model'+str(fold_no)+'.h5'

    model_checkpoint = ModelCheckpoint(filepath=mc_path, 
                                       monitor='val_loss', 
                                       mode='min', 
                                       save_best_only=True, 
                                       verbose=1)
    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)
    csv_logger = CSVLogger('path to the folder/train_log/training_log'+str(fold_no)+".csv", separator = ",", append=True)
    tensor_board = TensorBoard(log_dir = 'path to the folder/tensorboard_logs/log'+str(fold_no), 
                                           histogram_freq = 1, write_graph = True, write_images = True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)

    callback_list = [model_checkpoint, early_stop, csv_logger, tensor_board, reduce_lr]
    
    print('--------------------------------------------------')
    print(f'Training for fold number {fold_no} .....')
    
    history = model.fit(
    aug.flow(x_tr, y_tr, batch_size=BS),
    validation_data=(x_val, y_val),
    steps_per_epoch=len(x_tr) // BS,
    epochs=epochs, verbose=1,
    callbacks=callback_list
    )
    
    scores = model.evaluate(x_val, y_val, verbose=0)
    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')
    acc_per_fold.append(scores[1] * 100)
    loss_per_fold.append(scores[0])
    
    fold_no = fold_no + 1

#Printing average scores for cross validation
print('------------------------------------------------------------------------')
print('Score per fold')
for i in range(0, len(acc_per_fold)):
  print('------------------------------------------------------------------------')
  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')
print(f'> Loss: {np.mean(loss_per_fold)}')
print('------------------------------------------------------------------------')

#Model Prediction 
path = 'load saved models from the path'
prediction = list()
for model in os.listdir(path):
    new_model = load_model(path+model)
    pred = new_model.predict(x_test)
    prediction.append(pred)

#Prediction for each model
cv_pred = np.array([pred for pred in prediction])

#Average Prediction for cross validation
cv_pred_avg = np.mean(cv_pred, axis=0)
print("cv_pred_avg:", cv_pred_avg.shape)

predicted_class = np.argmax(cv_pred_avg, axis=1)
print("predicted_class:", predicted_class)

#Score Metrics
y_test_class = np.argmax(y_test,axis=1)
print("y_test_class:", y_test_class)
print(y_test_class.shape, predicted_class.shape)

#Classification report is saved in csv file format
report = metrics.classification_report(y_test_class, predicted_class, output_dict=True)
df = pd.DataFrame(report).transpose()
df.to_csv('vgg16_classification_report.csv', index = True)

#Receiver Characteristic Operating Curve values savd in csv file
fpr, tpr, threshold = roc_curve(y_test_class, cv_pred_avg[:,1], drop_intermediate=False)
roc_auc = metrics.auc(fpr, tpr)
roc_df = pd.DataFrame({'False Positive Rate': fpr, 'True Positive Rate': tpr, 'Thresholds': threshold})
roc_df.to_csv('vgg16_roc_curve.csv', index=True)

#Confusion matrix csv file
cm = metrics.confusion_matrix(y_test_class, predicted_class)
conf_mat = pd.DataFrame(cm)
conf_mat.to_csv('vgg16_conf_matrix.csv')

accuracy = accuracy_score(y_test_class, predicted_class)
print('Accuracy : ', accuracy)

precision = precision_score(y_test_class, predicted_class)
print('Precision score : ', precision)

recall = recall_score(y_test_class, predicted_class)
print('Recall score : ', recall)

f1 = f1_score(y_test_class, predicted_class)
print('F1-score : ', f1)
