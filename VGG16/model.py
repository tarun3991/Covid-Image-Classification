# -*- coding: utf-8 -*-
"""Final-Covid_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JKKlP3U3wC35L3JGo-ngLFRlZb49Eow-
"""

#Import tensorflow
import tensorflow as tf
print(tf.__version__)

physical_devices = tf.config.experimental.list_physical_devices('GPU')
print("physical_devices-------------", len(physical_devices))
tf.config.experimental.set_memory_growth(physical_devices[0], True)
tf.config.experimental.set_memory_growth(physical_devices[1], True)


#Importing the necessary packages
import os
import numpy as np
import pandas as pd
import cv2 as cv
import matplotlib.pyplot as plt
import keras
from keras.models import Sequential
from keras.layers.convolutional import Conv2D
from keras.layers import GlobalAveragePooling2D
from keras.layers.core import Activation, Dropout, Dense
from keras import backend as K
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam
from keras.preprocessing import image
from tensorflow.keras.utils import to_categorical
from keras.applications.vgg16 import VGG16
from keras.layers import Input
from keras.models import Model
from keras.models import load_model
import seaborn as sns
import sklearn.metrics as metrics
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score
from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau, TensorBoard


#Specifying learning rate, epochs and batch size
lr = 0.00001   
epochs = 500
BS = 32


#Dataset loading and preprocessing it
CATEGORIES = ["covid", "non-covid"]

def get_dataset(directory):
  data = []
  labels = []
  for category in CATEGORIES:
      path = os.path.join(directory, category)
      for img in os.listdir(path):
          img_path = os.path.join(path,img)
          im = cv.imread(img_path)
          im = cv.resize(im, (224,224))
          im = np.array(im)/255.0
          
          data.append(im)
          labels.append(category)

  lb = LabelBinarizer()
  labels = lb.fit_transform(labels)
  labels = to_categorical(labels) 
  n_classes = len(lb.classes_)

  data = np.array(data, dtype="float32") 
  labels = np.array(labels)

  return data, labels


#Training and Testing data  
directory = '/home/container/tarun/final_dl_work/images/train/'
x_train, y_train = get_dataset(directory)
x_test, y_test = get_dataset('/home/container/tarun/final_dl_work/images/test/')


#Data Augmentation
aug = ImageDataGenerator(
    rotation_range=25, width_shift_range=0.1,
    height_shift_range=0.1, shear_range=0.2, 
    zoom_range=0.2,horizontal_flip=True, 
    fill_mode="nearest")


#Five fold cross validation and model compiling and training
n_folds = 5
acc_per_fold = []
loss_per_fold = []
fold_no = 1

kf = KFold(n_splits=n_folds, shuffle=True, random_state=0)

for i, (tr_idx, val_idx) in enumerate(kf.split(x_train,y_train)):
    #print(len(tr_idx), len(val_idx))
    x_tr, y_tr = x_train[tr_idx], y_train[tr_idx]
    x_val, y_val = x_train[val_idx], y_train[val_idx]

    basemodel = VGG16(include_top=False, weights='imagenet', input_shape=(224,224,3))
    headmodel = basemodel.output
    headmodel = GlobalAveragePooling2D()(headmodel)
    headmodel = Dense(512, activation="relu")(headmodel)
    headmodel = Dropout(0.5)(headmodel)
    headmodel = Dense(64, activation="relu")(headmodel)
    headmodel = Dropout(0.5)(headmodel)
    headmodel = Dense((2), activation="softmax")(headmodel)

    model = Model(inputs=basemodel.input, outputs=headmodel)
    
    for layer in basemodel.layers:
        layer.trainable = False
        
    opt = Adam(learning_rate=lr, decay=lr/epochs)
    model.compile(optimizer = opt, loss='binary_crossentropy', metrics=["accuracy"])
    
    mc_path = '/home/container/tarun/final_dl_work/VGG16Model/vgg16-models/model'+str(fold_no)+'.h5'

    model_checkpoint = ModelCheckpoint(filepath=mc_path, 
                                       monitor='val_loss', 
                                       mode='min', 
                                       save_best_only=True, 
                                       verbose=1)
    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)
    csv_logger = CSVLogger('/home/container/tarun/final_dl_work/VGG16Model/train_log/training_log'+str(fold_no)+".csv", separator = ",", append=True)
    tensor_board = TensorBoard(log_dir = '/home/container/tarun/final_dl_work/VGG16Model/tensorboard_logs/log'+str(fold_no), 
                                           histogram_freq = 1, write_graph = True, write_images = True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1)

    callback_list = [model_checkpoint, early_stop, csv_logger, tensor_board, reduce_lr]
    
    print('--------------------------------------------------')
    print(f'Training for fold number {fold_no} .....')
    
    history = model.fit(
    aug.flow(x_tr, y_tr, batch_size=BS),
    validation_data=(x_val, y_val),
    steps_per_epoch=len(x_tr) // BS,
    epochs=epochs, verbose=1,
    callbacks=callback_list
    )
    
    scores = model.evaluate(x_val, y_val, verbose=0)
    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')
    acc_per_fold.append(scores[1] * 100)
    loss_per_fold.append(scores[0])
    
    fold_no = fold_no + 1


#Printing average scores for cross validation
print('------------------------------------------------------------------------')
print('Score per fold')
for i in range(0, len(acc_per_fold)):
  print('------------------------------------------------------------------------')
  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')
print(f'> Loss: {np.mean(loss_per_fold)}')
print('------------------------------------------------------------------------')


#Model Prediction 
path = '/home/container/tarun/final_dl_work/VGG16Model/vgg16-models/'
prediction = list()
for model in os.listdir(path):
    #print(model)
    new_model = load_model(path+model)
    pred = new_model.predict(x_test)
    prediction.append(pred)


#Prediction for each model
cv_pred = [np.array(pred[:,1] > 0.5, dtype=np.int) for pred in prediction]
#cv_pred


#Average Prediction for cross validation
cv_pred_avg = np.array(np.mean(prediction, axis=0)[:,1] > 0.5).astype(np.int)
#cv_pred_avg


#Score Metrics
accuracy_score = accuracy_score(y_test.argmax(axis=1), cv_pred_avg)
print('Accuracy : ', accuracy_score)

f1_score = f1_score(y_test.argmax(axis=1), cv_pred_avg)
print('F1-score : ', f1_score)

precision_score = precision_score(y_test.argmax(axis=1), cv_pred_avg)
print('Precision score : ', precision_score)

recall_score = recall_score(y_test.argmax(axis=1), cv_pred_avg)
print('Recall score : ', recall_score)


#Plotting ROC Curve
fpr, tpr, threshold = roc_curve(y_test.argmax(axis=1), cv_pred_avg)
roc_auc = metrics.auc(fpr, tpr)
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)
plt.legend(loc='lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.savefig("/home/container/tarun/final_dl_work/VGG16Model/plots/ROC_VGG16.png",dpi=600,format='png')
plt.clf()


#Plotting Confusion Matrix
cm = confusion_matrix(y_test.argmax(axis=1), cv_pred_avg)
axes = sns.heatmap(cm, annot=True, cbar=True, cmap=plt.cm.Blues, fmt='d')
axes.set_xlabel('Predicted')
axes.set_ylabel('Actual')
axes.set_title("confusion matrix")
axes.set_xticklabels(CATEGORIES)
axes.set_yticklabels(CATEGORIES)
plt.savefig("/home/container/tarun/final_dl_work/VGG16Model/plots/CM_VGG16.png",dpi=600,format='png')
plt.clf()


#Sensitivity and Specificity of the model
total1=sum(sum(cm))
sensitivity1 = cm[0,0]/(cm[0,0]+cm[0,1])
print('Sensitivity : ', sensitivity1 )

specificity1 = cm[1,1]/(cm[1,0]+cm[1,1])
print('Specificity : ', specificity1)
